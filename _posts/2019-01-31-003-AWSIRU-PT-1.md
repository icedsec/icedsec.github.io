---
layout: post
title: 'AWS + IR + U Part 1'
date: 2019-01-29
categories: aws,ir
---

# AWS + IR + U Part 1
This post walks through each phase of the security incident response process and explains actions we can take against our AWS account to be ready to detect and respond to incidents. We will cover settings or tools that you can apply to your whole account, or individual items within your AWS account.  

# Overview of incident response
If you're unfamilliar, incident response refers to exactly what it states - responding to incidents that have been identified within your organization. A broader defintion could include anything related to performance of systems within your organization but for the purposes of this post we are specifically referring to incidents that are related to security. 

**What do we mean by "incidents"?** 
This could be a myriad of different things such as credential compromise, a host has become infected, or evidence of intellectual property leaving the network unauthorized. It is also important to understand that there is a difference between a security event and a security incident - events have a smaller impact on the organization and do not require the full breadth of investigation or team composition that an incident does. That does not mean an event can't lead to an incident, however. For example: A phishing campaign on it's own might be a security event, but if during your analysis you discover that one user opened an attachment that contained a piece of malware that eventually stole credentials, then this might turn into an incident. 

**Okay, great. We know what incident response is. How do teams do it?**
A typical incident response team within an organization is made up of many different departments, and the team (under the command of an incdient handler) follows a pre-determined plan. Obviously, you can't plan for everything, but every incident response plan will have processes that fall within similar phases. These phases are based on frameworks such as ones from the SANS Institute or NIST, but some others exist as well. It's important to experiment and find out what works best for your organization, but I will always recommend those two frameworks as a starting point. 

For the purposes of this post, we're going to follow a hybrid of SANS and NIST SP 800-61. This hybrid was introduced to me in the Blue Team Handbook: Incident Response Edition. Also, because remediation and lessons learned will look different for every organization, we will not be covering those phases.



# Preparation
Preparation is the first phase in the incident response process and it is foundational to everything else that we're about to do. The reason for this is because the quality of every phase below this depends on how well prepared we were. 

Don't let that scare you - incident response is an iterative process and preparation is something that we'll feed back into once we've completed an incident. We'll touch up more on this later during our *Lessons Learned* discussion.



## Logging
Foundational to any security program, logging is truly what enables our ability to investigate and respond to incidents. Without proper logging we may not be able to properly detect, analyze, contain, eradicate or remediate threats from our envrionment. The reason for this is because if we don't know what happened or the method of which it happened by - how can we truly be sure it's no longer in our environment? AWS offers a wide array of options for logging. Let's take a look at some essentials. 



**CloudTrail** 

*CloudTrail* is the AWS service that logs all of the API calls (remember that everything you or a service does within AWS is an API call) that take place within your account. You can view the CloudTrail dashboard by logging into your account and choosing *CloudTrail*. This service is enabled and retains the last ninety days worth of events by default. 
        
Within the CloudTrail section of the AWS console, you might notice that you have the option to create *Trails*. When creating a trail you get to decide what region the trail will collect the events from, what events to collect, and where the events are stored.  
        
Best practices related to CloudTrail:
- Create a trail that applies to all regions, captures all management/data events, and is stored in a sufficiently secured S3 bucket that belongs to a separate account (If you use AWS Organizations you can expand this to be an Organizational Trail)
- The "sufficiently secured" S3 bucket should have proper permissions, object logging, MFA-delete and encryption enabled
- Encrypt and enable validation on files sent as part of Trails (Done within the "Advanced" settings of the Trail)
- Configure a CloudWatch Logs group for analysis and alerting of your CloudTrail logs (Done after the Trail has been created)
- Follow the [Amazon Documentation](https://docs.aws.amazon.com/awscloudtrail/latest/userguide/control-user-permissions-for-cloudtrail.html) related to IAM, S3, KMS, and SNS policies for managing the permissions of your CloudTrail-adjacent resources
- Send the CloudTrail logs into your centralized log management system, such as ELK, by using the *S3* or *File* Logstash input

Basic use cases for analysis:
- Keys used outside of region
- Root keys are used
- Specific EC2 instance terminated/instance type created
- IAM roles or policies being created or modified
- Specific S3 objects being accessed
- Changes to service configuration across the account

Obviously, these are just some simple examples of what you can do once you have your CloudTrail logs enabled and being shipped to your alerting platform. 



**CloudWatch**

CloudWatch is an AWS service that you can use to monitor your consumption of resources and your AWS-developed applications. From the console you can view different metrics (or custom data) about your services and even create your own dashboards. You can also send logs to CloudWatch using the CloudWatch Logs service. The most common use of this is sending your CloudTrail logs to CloudWatch for monitoring. CloudWatch Logs also features a newer subservice called Insights which allows you to run queries and create visualizations or alerts on the logs within CloudWatch Logs.

Using CloudWatch Alarms, the CloudWatch-generated metrics can be used to initate actions such as auto-scaling or auto-termination in the case of EC2 or they can also create alerts when a metric exceeds a threshold, such as a notification when your bill is at a certain value. AWS or custom services can use these alarms.  

CloudWatch Events allow you to invoke an action based on a specific pattern or schedule based on an event from a specific service. A common recommendation would be to send CloudTrail logs to CloudWatch and then create a CloudWatch Event to create an alert when DeleteTrail shows up in the CloudTrail logs. This concept can be expanded to other services such as EC2 (e.g. InstanceTerminated) or IAM (e.g. CreateAccessKey).

It is important to ensure that IAM policies are correctly set up surrounding CloudWatch. This not only includes the users that will be usings this information, but all other consumers of this information such as monitoring tools. Amazon has some documentation on this topic outlining the best practices [here](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/iam-identity-based-access-control-cw.html). 


CloudTrail and CloudWatch make up the bulk of the importance on being prepared for an incident, so for the next several topics we will not be going into as much detail. 



**VPC Flows**

*VPC Flow Logging* is essentialy netflow logs within AWS. They capture metadata about network traffic that happens within that VPC, including traffic coming in or leaving that VPC. Without these logs, you essentially have no visibility into the network traffic that is happening with your VPC. 

Enabling the VPC logs is very straight-forward and highly recommended. To enable your flow logs, you can:
    1. Sign into your AWS Console
    2. From the *Services* menu, enter 'VPC'
    3. Click *Your VPCs* under the *Virtual Private Cloud* heading
    4. Choose your VPC and then click on the *Flow Logs* tab
    5. Click *Create flow log*
    6. Choose your desired settings. We'd recommend that you choose *All* for the filter setting, and send them to CloudWatch, ensuring you're using the proper IAM role for your desired setting.



**S3 Access and Object-level Logs**

When you set up an S3 bucket, you also have the option to turn on different types of logging. You can enable *Access* logging and *Object-level* logging. 

With *Access* logging, you log each request that is being made to the bucket. This includes items such as what was being requested and how. Think of these logs as akin to web server logs.

*Object-level* logging tracks the API calls made to the bucket and places them into CloudTrail. 

It is recommended that you have each of these logging mechanisms enabled on sensitive buckets.



**ELB Access Logging**

If you're using AWS Elastic Load Balancers for any sort of load balancing, it is important to ensure that you have these logs on in order to gain visibility into the requests made to the resources behind the ELBs. These requests are again in CLF and similar to what you'd expect from a web server. 

[Follow this Amazon documentation](https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/enable-access-logs.html#attach-bucket-policy) in order to get yourself set up. 




## IAM Policies and Roles

A big part of preparation lies within Identity and Access Management (IAM). IAM is what dictates who has access to what, and what they're allowed to do, when they can do it, and for how long.  This is controlled in AWS through the IAM Management Console (Accessed via *Home* and then searching for *IAM*). 

If this is your first time using IAM within AWS, we should define some terms first. 

*User* - An individual user, such as Bob. Bob can be created in the IAM console to have Console (web UI) access or programmatic access (think API keys). If you have programmtic access you are supplied with two keys - an Access key and a Secret key. You always need both to access anything, and as the name implies, do not share your secret key. You can read more about those keys [here](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html).

*Group* - A collection of users. 

*Role* - This is used to represent something other than a group or user become the subject of receiving a policy. A basic example of this would be an EC2 instance. Your EC2 instance can "assume" a role, provided it has the permissions to do so. Once it has assumed the role, it then has the same policies applied to it that the role does. 

*Policy* - These allow us to define the access to the resources above. At first, they can be a little confusing to read, but let's break one down. You'll see they aren't so bad after all. 
*Principal* - The principal dictates what user, group, or role the policy applies to.
*Action* - The action defines what the principal is allowed to do.
*Resource* - The resource refers to what service the actions of the principal can take place on.
*Condition* - Allows us to give a bit of control around when these permissions will apply. This could be something such as an IP comparison or timerange.



**Example Policies**

Here's an example policy that allows us read-only access to all S3 buckets in an account:

```{
"Version": "2012-10-17",
"Statement": [
    {
        "Effect": "Allow",
        "Action": [
            "s3:Get*",
            "s3:List*"
        ],
        "Resource": "*"
    }
]
}
```



Hmm... That's too permissive. Let's dial that back so that we only allow access to a specific bucket. 

```{
"Version": "2012-10-17",
"Statement": [
    {
        "Effect": "Allow",
        "Action": [
            "s3:Get*",
            "s3:List*"
            ],
"Resource": [
        "arn:aws:s3:::bucke_name_here/*",
        "arn:aws:s3:::bucket_name_here"
    ],
"Condition": {
    "BoolIfExists": {
        "aws:MultiFactorAuthPresent": "true"
        }
}
    }
    ]
}
```


This policy defines that we will provide read-only access to a specific bucket and all objects within the bucket if MFA is present.

While there are many lists of best practices, including [Amazon's](https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html), some of our favourites include:
- Disabling & delete your root account keys
- Enabling multi-factor authentication for sensitive users (Console or API keys)
- Apply the principals of least privilege, take advantage of AWS AccessAdvisor to assist you with this
- Apply a strong password policy for users (16+ characters, alphanum, symbols, no reuse, etc.)
- Regularly audit your users and their access
- Avoid the use of in-line policies, focus on groups
- Prevent specific access from being created, such as Allow any Action over any Resource
- Have a game plan to rotate keys within your account - there is a good chance they will be accidentally leaked at some point



## Misc
This section will cover random ramblings about things that make up a foundational security program, and how they can be done with some neat Bezos magic. 



**Inventory**

Inventory is a pivotal part of any security program. The ability to track what is in your AWS account is very important when it comes to security or even controlling costs. How can you secure what you don't know exists? 

Here are some useful links that you can use to help find what's in your account or be alerted when something pops up:
https://github.com/Netflix/security_monkey
https://github.com/nccgroup/aws-inventory



**Forensics/backup account**

Having a forensic and/or backup AWS account is a wise idea. The forensic account can be used to place any evidence in when performing an investigation to ensure that things you're collecting or analyzing do not get mixed up with existing resources. In addition, having a separate account for any backups you are producing provides a layer of protection if everything in your account got destroyed.



**SOPs**

Having standard operating procedures for what to do when a certain event occurs within the environment ensures that no one is fumbling when a real investigation is underway. Common SOPs might include:
- Disk Imaging 
- Memory Acquisition
- Moving Evidence to Forensic Account for Analysis
- Set-up Analysis Environment

Creating scripts, Docker containers, SSM Commands, etc. can also help simplify SOPs and enable any analyst to produce repeatable results. 



**Before we go...**

Ultimately it comes down to what services youâ€™re using - there is no one size that fits all here. 

Join us in the next post where we continue down the other phases of the incident response process. 